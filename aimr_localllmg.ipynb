{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6383829c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import os\n",
    "import sentencepiece\n",
    "import hf_xet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ea7c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################### Generate note#################################################################################################\n",
    "# Load transcripts and save as global variables\n",
    "save_dir = \n",
    "n =10\n",
    "for i in range(1, n + 1):\n",
    "    filename = f'encounter_transcript_{i}.txt'\n",
    "    filepath = os.path.join(save_dir, filename)\n",
    "    if os.path.exists(filepath):\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            globals()[f'encounter_transcript_{i}'] = f.read()\n",
    "        print(f\"Loaded {filepath} into variable encounter_transcript_{i}\")\n",
    "    else:\n",
    "        print(f\"File {filepath} not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc405415",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = f\"\"\"You are a medical documentation assistant. Based on the following doctor-patient conversation, create a structured medical note with the following sections:\n",
    "\n",
    "1. History of Present Illness (HPI): should be in format of a list of problems with details for each problem in a short paragraph. Do not add other subheadings to organize the information.\n",
    "Problem 1: HPI Details\n",
    "Problem 2: HPI Details\n",
    "Problem n: HPI details\n",
    "2. Visit Diagnoses\n",
    "-Problem 1\n",
    "-Problem 2\n",
    "-Problem n  \n",
    "3. Assessment and Plan\n",
    "Problem 1\n",
    "-Assessment:\n",
    "-Plan:\n",
    "Problem 2\n",
    "-Assessment:\n",
    "-Plan:\n",
    "Problem n\n",
    "-Assessment:\n",
    "-Plan:\n",
    "4. Orders #This section should only include orders that would need to be placed in the EMR system. Include medications, labs, imaging, referrals, etc. Do not include any other information such as counseling.\n",
    "Problem 1\n",
    "-Order 1\n",
    "-Order 2\n",
    "-Order n\n",
    "Problem 2\n",
    "-Order 1\n",
    "-Order 2\n",
    "-Order n\n",
    "Problem n\n",
    "-Order 1\n",
    "-Order 2\n",
    "-Order n\n",
    "Please format the output with clear section headers as detailed above without addition of other subheadings/sections and professional medical documentation style.\\n\"\"\"\n",
    "\n",
    "footer = \"Medical note:\\n\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc7438e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Qwen-1.5B\n",
    "model_name = \"Qwen/Qwen2.5-1.5B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dbe273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move model to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c5a92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revised prompting\n",
    "for i in range(1, n + 1):\n",
    "    transcript_content = globals()[f'encounter_transcript_{i}']\n",
    "    prompt = f\"{instructions}\\n\\nTranscript:\\n{transcript_content}\\n\\n{footer}\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=False)\n",
    "    \n",
    "    # Move inputs to the same device as the model (GPU)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=1200,  # Uncomment this to limit output length\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    # Decode only the new tokens (skip the input prompt)\n",
    "    response_text = tokenizer.decode(outputs[0][len(inputs['input_ids'][0]):], skip_special_tokens=True)\n",
    "    globals()[f'encounter_summary_{i}_{model_name[5:]}'] = response_text\n",
    "    print(f\"Generated encounter_summary_{i}_{model_name[5:]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb31af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir_summ = \n",
    "os.makedirs(save_dir_summ, exist_ok=True)\n",
    "for i in range(1, n+1):\n",
    "    # Save the summary to a file\n",
    "    filename = f'encounter_summary_{i}_{model_name[5:]}.txt'\n",
    "    filepath = os.path.join(save_dir_summ, filename)\n",
    "    \n",
    "    # Delete existing file if it exists\n",
    "    if os.path.exists(filepath):\n",
    "        os.remove(filepath)\n",
    "    \n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"{filename}:\\n\\n\")\n",
    "        f.write(globals()[f'encounter_summary_{i}_{model_name[5:]}'])\n",
    "    print(f\"Saved {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6e9d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU memory - run this after your generation loop\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "# Delete model and tokenizer from memory\n",
    "if 'model' in globals():\n",
    "    del model\n",
    "if 'tokenizer' in globals():\n",
    "    del tokenizer\n",
    "\n",
    "# Clear any input tensors that might be lingering\n",
    "if 'inputs' in globals():\n",
    "    del inputs\n",
    "if 'outputs' in globals():\n",
    "    del outputs\n",
    "\n",
    "# Force garbage collection\n",
    "gc.collect()\n",
    "\n",
    "# Clear GPU cache if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"GPU memory cleared\")\n",
    "    print(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "    print(f\"GPU memory reserved: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"CPU memory cleared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1376f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Qwen-3B\n",
    "\n",
    "# Configure 8-bit quantization\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    bnb_8bit_compute_dtype=torch.float16,\n",
    "    bnb_8bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "# load Qwen-3B in int8\n",
    "model_name = \"Qwen/Qwen2.5-3B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    quantization_config=quantization_config,\n",
    "    #device_map=\"auto\"  # Automatically handle device placement\n",
    ")\n",
    "\n",
    "print(f\"{model_name} loaded in 8-bit quantization\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4013659",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
